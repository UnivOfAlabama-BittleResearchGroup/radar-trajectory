{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Lanes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import utm\n",
    "\n",
    "repo_root = Path(os.getcwd()).parent\n",
    "\n",
    "while not (repo_root / \".git\").exists():\n",
    "    repo_root = repo_root.parent\n",
    "\n",
    "sys.path.append(str(repo_root))\n",
    "\n",
    "# set the pandas plotting backend to plotly\n",
    "pd.options.plotting.backend = \"plotly\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Radar Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the Data\n",
    "from src.filtering import Filtering\n",
    "\n",
    "\n",
    "df = pl.scan_parquet(\n",
    "    \"/Users/max/Library/CloudStorage/Box-Box/Radar-Data/new_format/167865*.parquet\"\n",
    ").collect()\n",
    "\n",
    "\n",
    "# create the file paths\n",
    "network_outline_file = repo_root / \"geo_data\" / \"network_outline.geojson\"\n",
    "radar_locations_file = repo_root / \"geo_data\" / \"radar_origins.json\"\n",
    "\n",
    "f = Filtering(\n",
    "    radar_location_path=radar_locations_file,\n",
    "    network_boundary_path=network_outline_file,\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df.pipe(f.create_object_id)\n",
    "    # resample to 10 Hz\n",
    "    .pipe(f.resample, 100)\n",
    "    # remove objects that travel for very little time\n",
    "    .pipe(f.filter_short_trajectories, minimum_distance_m=100, minimum_duration_s=5)\n",
    "    # clip trajectories to not include the constant speed at the end\n",
    "    .pipe(f.clip_trajectory_end)\n",
    "    # rotate the heading measurements to world coordinates\n",
    "    .pipe(f.rotate_heading)\n",
    "    # update the centroid coordinates to the actual center of the object\n",
    "    .pipe(\n",
    "        f.correct_center,\n",
    "        x_col=\"utm_x\",\n",
    "        y_col=\"utm_y\",\n",
    "    )\n",
    "    # convert the h3 integer to a string\n",
    "    .pipe(f.int_h3_2_str)\n",
    "    # filter out objects that are not in the network\n",
    "    .pipe(f.filter_network_boundaries)\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Look at One TL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_df = df.filter(pl.col(\"ip\") == \"10.160.7.137\")\n",
    "\n",
    "f.h3_resolution = 14\n",
    "\n",
    "interest_df = interest_df.pipe(f.radar_to_h3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the x/y data in a scatter plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "_df = interest_df.with_columns(\n",
    "    [\n",
    "        pl.col(\"object_id\").n_unique().over(\"h3\").alias(\"n_objects\"),\n",
    "    ]\n",
    ").filter(pl.col(\"n_objects\") > 10)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=_df[\"utm_x\"],\n",
    "        y=_df[\"utm_y\"],\n",
    "        mode=\"markers\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# make the image square\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    # width=800,\n",
    "    # height=800,\n",
    "    xaxis=dict(\n",
    "        scaleanchor=\"y\",\n",
    "        scaleratio=1,\n",
    "    ),\n",
    ")\n",
    "# plot with a white background and no x and y axis\n",
    "fig.update_layout(\n",
    "    plot_bgcolor=\"white\",\n",
    "    xaxis_showgrid=False,\n",
    "    yaxis_showgrid=False,\n",
    "    xaxis_zeroline=False,\n",
    "    yaxis_zeroline=False,\n",
    "    xaxis_visible=False,\n",
    "    yaxis_visible=False,\n",
    ")\n",
    "\n",
    "# make figure span the whole figure\n",
    "fig.update_layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0,\n",
    "        pad=0,\n",
    "    )\n",
    ")\n",
    "\n",
    "# write to a huge figure\n",
    "fig.write_image(\n",
    "    repo_root / \"lane-finder.png\",\n",
    "    width=2400,\n",
    "    height=2400,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_df[\"epoch_time\"].max() - interest_df[\"epoch_time\"].min()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Grid Distance Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba as nb\n",
    "import h3\n",
    "\n",
    "\n",
    "def gen_cust_dist_func(parallel=True):\n",
    "    def cust_dot_T(A, B):\n",
    "        assert B.shape[1] == A.shape[1]\n",
    "\n",
    "        out = np.empty((A.shape[0], B.shape[0]), dtype=A.dtype)\n",
    "        for i in nb.prange(A.shape[0]):\n",
    "            for j in range(B.shape[0]):\n",
    "                acc = 0\n",
    "                for k in range(A.shape[1]):\n",
    "                    acc += h3.h3_distance(A[i, k], B[j, k])\n",
    "                out[i, j] = np.mean(acc)\n",
    "        return out\n",
    "\n",
    "    # if parallel==True:\n",
    "    #     return nb.njit(cust_dot_T,fastmath=True,parallel=True)\n",
    "    # else:\n",
    "    #     return nb.njit(cust_dot_T,fastmath=True,parallel=False\n",
    "    # r)\n",
    "    return cust_dot_T\n",
    "\n",
    "\n",
    "cust_dist_func = gen_cust_dist_func(parallel=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform, cdist\n",
    "\n",
    "\n",
    "def get_h3_distance_matrix(h3_list):\n",
    "    \"\"\"Calculate the distance matrix between all h3 indices in a list\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    h3_list : list\n",
    "        list of h3 indices\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        distance matrix\n",
    "    \"\"\"\n",
    "    return pdist(\n",
    "        np.array(h3_list).reshape(-1, 1),\n",
    "        np.array(h3_list).reshape(-1, 1),\n",
    "        metric=lambda x, y: h3.h3_distance(str(x[0]), str(y[0])),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = get_h3_distance_matrix(h3s)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compress Each Trajectory into it's H3 representation\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New idea:\n",
    "\n",
    "- Compress each trajectory into h3 representation\n",
    "- The a trajectory is 5 steps of h3\n",
    "- Use the center coords of h3 to cluster the trajectories (using euclidean distance)\n",
    "- The distance can be precomputed using h3 center -> utm x,y ->\n",
    "-\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Unique H3 -> X,Y mapping & Then a Distance Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utm\n",
    "import h3\n",
    "import numpy as np\n",
    "\n",
    "map_df = (\n",
    "    interest_df.groupby([\"object_id\", \"h3\"])\n",
    "    .first()\n",
    "    .sort(\"epoch_time\")\n",
    "    .select([pl.col(\"h3\").unique()])\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col(\"h3\")\n",
    "            .apply(\n",
    "                lambda x: h3.h3_to_geo(\n",
    "                    x,\n",
    "                ),\n",
    "            )\n",
    "            .alias(\"latlon\"),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col(\"latlon\").apply(lambda x: utm.from_latlon(x[0], x[1])).alias(\"utm\"),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns([pl.col(\"utm\").apply(lambda x: list(x[:2])).alias(\"utm_coords\")])\n",
    "    .drop([\"utm\", \"latlon\"])\n",
    "    # .to_pandas()\n",
    ")\n",
    "\n",
    "\n",
    "point_array = np.array(map_df[\"utm_coords\"].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the point distance to a square matrix of distances\n",
    "dist = pdist(point_array, metric=\"euclidean\")\n",
    "dist = squareform(dist)\n",
    "\n",
    "# get the h3 indices\n",
    "h3s = map_df[\"h3\"].to_list()\n",
    "\n",
    "# add the h3 indices to the distance matrix\n",
    "dist_df = pd.DataFrame(dist, columns=h3s, index=h3s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_sequence_length = 10\n",
    "\n",
    "sub_trajectories = (\n",
    "    interest_df\n",
    "    #  create a running count of each vehicle index\n",
    "    .with_columns([pl.col(\"h3\").cumcount().over(\"object_id\").alias(\"vehicle_index\")])\n",
    "    .groupby(\"object_id\", pl.col(\"vehicle_index\") // sub_sequence_length)\n",
    "    .agg([pl.col(\"h3\")])\n",
    "    .with_columns([pl.col(\"h3\").arr.join(\"-\")])\n",
    "    .groupby(pl.col(\"h3\"))\n",
    "    .count()\n",
    "    .filter(pl.col(\"count\") > 1)\n",
    "    .with_columns([pl.col(\"h3\").str.split(\"-\").alias(\"h3s\")])\n",
    "    .explode(\"h3s\")\n",
    "    .join(map_df, left_on=\"h3s\", right_on=\"h3\")\n",
    "    .groupby(\"h3\")\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"h3s\"),\n",
    "            pl.col(\"utm_coords\"),\n",
    "        ]\n",
    "    )\n",
    "    .select([pl.col(\"h3\"), pl.col(\"h3s\"), pl.col(\"utm_coords\")])\n",
    "    .filter(pl.col(\"h3s\").arr.lengths() == sub_sequence_length)\n",
    ")\n",
    "\n",
    "sub_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import h3\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def euclidean_distance(points1, points2):\n",
    "    return np.sqrt(((points1 - points2) ** 2).sum(axis=1)).mean()\n",
    "\n",
    "\n",
    "# @jit(nopython=True)\n",
    "def compute_distances(pairs):\n",
    "    num_pairs = pairs.shape[0]\n",
    "    combinations = [(i, j) for i in range(num_pairs) for j in range(i + 1, num_pairs)]\n",
    "    res = np.empty(len(combinations))\n",
    "\n",
    "    for k, (p1, p2) in enumerate(combinations):\n",
    "        for z1, z2 in zip(pairs[p1], pairs[p2]):\n",
    "            res[k] += h3.h3_distance(str(z1), str(z2))\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "# pairs = np.array([k for v in sub_trajectories['utm_coords'].to_list() for k in v]).reshape(len(sub_trajectories['utm_coords']), 5, 2)\n",
    "pairs = np.array([v for v in sub_trajectories[\"h3s\"].to_list()])\n",
    "\n",
    "res = compute_distances(pairs)\n",
    "res = squareform(res)\n",
    "\n",
    "distance_df = pd.DataFrame(\n",
    "    res,\n",
    "    columns=sub_trajectories[\"h3\"].to_list(),\n",
    "    index=sub_trajectories[\"h3\"].to_list(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of the combinations\n",
    "distance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_obj_ids = (\n",
    "    interest_df\n",
    "    # every 3 hexagons, create a new object id\n",
    "    .groupby_dynamic(\"epoch_time\", every=\"2s\", by=\"object_id\")\n",
    "    .agg([pl.col(\"h3\")])\n",
    "    .with_row_count()\n",
    "    .with_columns(\n",
    "        [(pl.col(\"object_id\") + \"-\" + pl.col(\"row_nr\").cast(str)).alias(\"object_id\")]\n",
    "    )\n",
    "    .drop([\"row_nr\", \"epoch_time\"])\n",
    "    .explode(\"h3\")\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col(\"object_id\").count().over(\"h3\").alias(\"count\"),\n",
    "        ]\n",
    "    )\n",
    "    .filter(pl.col(\"count\") > 2)\n",
    "    .drop(\"count\")\n",
    ")\n",
    "\n",
    "\n",
    "h3_dataframe = (\n",
    "    h3_obj_ids.pivot(\n",
    "        values=\"h3\",\n",
    "        index=\"object_id\",\n",
    "        columns=\"h3\",\n",
    "        aggregate_function=\"count\",\n",
    "    )\n",
    "    .fill_null(0)\n",
    "    .to_pandas()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3s = h3_dataframe.set_index(\n",
    "    \"object_id\",\n",
    ").columns.to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do tsne on the h3 dataframe\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, verbose=1, metric=\"cosine\")\n",
    "\n",
    "tsne_results = tsne.fit_transform(h3_dataframe.drop(columns=\"object_id\").to_numpy())\n",
    "\n",
    "# merge the tsne results with the original dataframe\n",
    "tsne_df = pd.DataFrame(tsne_results, columns=[\"tsne_x\", \"tsne_y\"])\n",
    "\n",
    "tsne_df = pd.concat([h3_dataframe[\"object_id\"], tsne_df], axis=1)\n",
    "\n",
    "# plot the tsne results\n",
    "fig = go.Figure()\n",
    "\n",
    "\n",
    "_plot_df = tsne_df.sample(1000)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=_plot_df[\"tsne_x\"],\n",
    "        y=_plot_df[\"tsne_y\"],\n",
    "        mode=\"markers\",\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering on Sub Trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from scipy.spatial import ConvexHull\n",
    "import numpy as np\n",
    "import h3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = h3_dataframe.drop(columns=\"object_id\").T.copy()\n",
    "\n",
    "dbscan = DBSCAN(eps=20, min_samples=5, metric=\"precomputed\")\n",
    "dbscan.fit(\n",
    "    distance_df,\n",
    ")\n",
    "\n",
    "dbscan.labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.DataFrame(dbscan.labels_, columns=[\"label\"], index=distance_df.index)\n",
    "# reset the index, split the h3s\n",
    "label_df = label_df.reset_index().rename(columns={\"index\": \"h3\"})\n",
    "label_df[\"h3s\"] = label_df[\"h3\"].str.split(\"-\")\n",
    "label_df = label_df.explode(\"h3s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the clusters\n",
    "fig = go.Figure()\n",
    "\n",
    "for cluster, _df in label_df.groupby(\"label\"):\n",
    "    # if cluster == -1:\n",
    "    #     continue\n",
    "    # get the set of h3 cells for the cluster\n",
    "    h3s = _df[\"h3s\"].unique().tolist()\n",
    "\n",
    "    # plot the extend of the h3 cells\n",
    "    geometry = h3.h3_set_to_multi_polygon(h3s, geo_json=True)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scattermapbox(\n",
    "            lon=[x[0] for x in geometry[0][0]],\n",
    "            lat=[x[1] for x in geometry[0][0]],\n",
    "            mode=\"lines\",\n",
    "            name=f\"cluster {cluster}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # fig.add_trace(\n",
    "    #     go.Scatter(\n",
    "    #         x=data[\"f32_positionX_m\"],\n",
    "    #         y=data[\"f32_positionY_m\"],\n",
    "    #         mode=\"markers\",\n",
    "    #         name=f\"cluster {cluster}\",\n",
    "    #     )\n",
    "    # )\n",
    "\n",
    "# make the plot square\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=800,\n",
    "    autosize=False,\n",
    "    showlegend=True,\n",
    "    xaxis=dict(\n",
    "        scaleanchor=\"y\",\n",
    "        scaleratio=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# add mapbox\n",
    "fig.update_layout(\n",
    "    # use mapbox for base tiles\n",
    "    mapbox_style=\"stamen-terrain\",\n",
    "    # center on Tuscaloosa\n",
    "    mapbox_center_lat=33.2,\n",
    "    mapbox_center_lon=-87.5,\n",
    "    mapbox_zoom=10,\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering on H3 Vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne_df[tsne_df[\"cluster\"] == cluster,].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluster using DBSCAN\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering\n",
    "from scipy.spatial import ConvexHull\n",
    "import numpy as np\n",
    "import h3\n",
    "\n",
    "\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=7, metric=\"cosine\")\n",
    "dbscan.fit(h3_dataframe.drop(columns=\"object_id\"))\n",
    "tsne_df[\"cluster\"] = dbscan.labels_\n",
    "\n",
    "# create a dictionary of included hexagons\n",
    "\n",
    "cluster_h3s = {}\n",
    "\n",
    "for cluster in tsne_df[\"cluster\"].unique():\n",
    "    if cluster >= 0:\n",
    "        h3s = (\n",
    "            h3_obj_ids.filter(\n",
    "                pl.col(\"object_id\").is_in(\n",
    "                    tsne_df.loc[tsne_df[\"cluster\"] == cluster, \"object_id\"].to_list()\n",
    "                )\n",
    "            )[\"h3\"]\n",
    "            .unique()\n",
    "            .to_list()\n",
    "        )\n",
    "\n",
    "        cluster_h3s[cluster] = set(h3s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the clusters\n",
    "fig = go.Figure()\n",
    "\n",
    "for cluster, h3s in cluster_h3s.items():\n",
    "    if cluster == -1:\n",
    "        continue\n",
    "\n",
    "    # if cluster not in [57, 18, 43, 11, 73, 42, 48]:\n",
    "    #     continue\n",
    "\n",
    "    # plot the extend of the h3 cells\n",
    "    geometry = h3.h3_set_to_multi_polygon(h3s, geo_json=True)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scattermapbox(\n",
    "            lon=[x[0] for x in geometry[0][0]],\n",
    "            lat=[x[1] for x in geometry[0][0]],\n",
    "            mode=\"lines\",\n",
    "            name=f\"cluster {cluster}\",\n",
    "            fill=\"toself\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "# make the plot square\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=800,\n",
    "    autosize=False,\n",
    "    showlegend=True,\n",
    "    xaxis=dict(\n",
    "        scaleanchor=\"y\",\n",
    "        scaleratio=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# add mapbox\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"open-street-map\",\n",
    "    # center on Tuscaloosa\n",
    "    mapbox_center_lat=33.2,\n",
    "    mapbox_center_lon=-87.5,\n",
    "    mapbox_zoom=10,\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_h3s = (\n",
    "    interest_df.groupby([\"h3\", \"object_id\"])\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"f32_velocityInDir_mps\").mean().alias(\"f32_velocityInDir_mps\"),\n",
    "            pl.col(\"epoch_time\").first().alias(\"epoch_time\"),\n",
    "        ]\n",
    "    )\n",
    "    .sort([\"object_id\", \"epoch_time\"])\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consume all Clusters completely in other Clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = list(cluster_h3s.keys())\n",
    "# overlap = {c: {v: 0} for c in clusters for v in clusters if c != v}\n",
    "\n",
    "interest_df = interest_df.sort([\"object_id\", \"epoch_time\"]).with_columns([])\n",
    "start_end = {}\n",
    "\n",
    "overlaps = []\n",
    "\n",
    "for cluster_1 in clusters:\n",
    "    for cluster_2 in clusters:\n",
    "        if (cluster_1 != cluster_2) and (\n",
    "            (cluster_1 in cluster_h3s) and (cluster_2 in cluster_h3s)\n",
    "        ):\n",
    "            c1_h3s = cluster_h3s.get(cluster_1, set())\n",
    "            c2_h3s = cluster_h3s.get(cluster_2, set())\n",
    "\n",
    "            if len(c1_h3s.intersection(c2_h3s)):\n",
    "                overlaps.append((cluster_1, cluster_2))\n",
    "\n",
    "            # try to consume the small ones.\n",
    "            if len(c1_h3s.intersection(c2_h3s)) >= len(c2_h3s):\n",
    "                cluster_h3s.pop(cluster_2, None)\n",
    "                print(f\"popping {cluster_2}\")\n",
    "            elif len(c2_h3s.intersection(c1_h3s)) >= len(c1_h3s):\n",
    "                cluster_h3s.pop(cluster_1, None)\n",
    "                print(f\"popping {cluster_1}\")\n",
    "\n",
    "#     if (cluster_1 in cluster_h3s) and (cluster_1 not in start_end):\n",
    "#         # build paths that include the cluster\n",
    "\n",
    "\n",
    "#     if (cluster_1 in cluster_h3s) and (cluster_1 not in start_end):\n",
    "\n",
    "#         # find the start and end of each segment by looking at vehicles in the cluster\n",
    "#         start_n_ends = (\n",
    "#             interest_df.filter(\n",
    "#                 pl.col('h3').is_in(list(cluster_h3s[cluster_1]))\n",
    "#             ).groupby([\n",
    "#                 'object_id'\n",
    "#             ]).agg([\n",
    "#                 pl.col('h3').first().alias('start'),\n",
    "#                 pl.col('h3').last().alias('end')\n",
    "#             ])\n",
    "#         )\n",
    "\n",
    "\n",
    "#         # take any hexagon which has 10% of the arrivals or departures\n",
    "#         starts = start_n_ends['start'].value_counts().with_columns([\n",
    "#             (pl.col('counts') / pl.col('counts').sum())\n",
    "#         ]).filter(\n",
    "#             pl.col('counts') > 0.1\n",
    "#         )['start'].to_list()\n",
    "\n",
    "#         ends = start_n_ends['end'].value_counts().with_columns([\n",
    "#             (pl.col('counts') / pl.col('counts').sum())\n",
    "#         ]).filter(\n",
    "#             pl.col('counts') > 0.1\n",
    "#         )['end'].to_list()\n",
    "\n",
    "#         start_end[cluster_1] = (set(starts), set(ends))\n",
    "\n",
    "\n",
    "# # create a distance matrix of overlap #\n",
    "\n",
    "# overlaps_counts = {}\n",
    "# for c1, c2 in overlaps:\n",
    "#     # first try c1->c2:\n",
    "#     s, e = start_end[c1]\n",
    "#     s2, e2 = start_end[c2]\n",
    "#     overlaps_counts[(c1, c2)] = len(e.intersection(s2))\n",
    "#     overlaps_counts[(c2, c1)] = e2.intersection(s)\n",
    "\n",
    "\n",
    "# # istead create a prob using vehicle transitions\n",
    "# # if a vehicle goes from 1 -> 2 -> 1, then discard 2 from its trajectory\n",
    "# # what to do if it is multiple at the same time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map the Label of Each Cluster to the Trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = (\n",
    "    vehicle_h3s.with_columns(\n",
    "        [\n",
    "            # pl.when(pl.col('h3').is_in(list(v))).then(pl.lit(str(c))).otherwise(None).alias(str(c)) for c, v in cluster_h3s.items()\n",
    "            pl.col(\"h3\")\n",
    "            .apply(lambda h3: [c for c, v in cluster_h3s.items() if h3 in v])\n",
    "            .alias(\"clusters\")\n",
    "        ]\n",
    "    )\n",
    "    .explode(\"clusters\")\n",
    "    .filter(pl.col(\"clusters\").is_not_null())\n",
    "    .groupby([\"object_id\", \"clusters\"])\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"epoch_time\").first().alias(\"start_time\"),\n",
    "            pl.col(\"epoch_time\").last().alias(\"end_time\"),\n",
    "            pl.col(\"h3\").len().alias(\"num_h3s\"),\n",
    "        ]\n",
    "    )\n",
    "    .sort([\"object_id\", \"start_time\", \"end_time\"])\n",
    "    .with_columns(\n",
    "        [\n",
    "            *(\n",
    "                pl.col(c)\n",
    "                .shift(1)\n",
    "                .backward_fill(1)\n",
    "                .over(\"object_id\")\n",
    "                .alias(f\"{c}_shifted\")\n",
    "                for c in [\"start_time\", \"end_time\"]\n",
    "            ),\n",
    "            (pl.col(\"end_time\") - pl.col(\"start_time\")).dt.seconds().alias(\"duration\"),\n",
    "            (\n",
    "                pl.col(\"num_h3s\")\n",
    "                / pl.col(\"clusters\").apply(lambda x: len(cluster_h3s[x]))\n",
    "            ).alias(\"percent_h3s\"),\n",
    "        ]\n",
    "    )\n",
    "    .filter(\n",
    "        (pl.col(\"end_time\") >= pl.col(\"end_time_shifted\"))\n",
    "        & (pl.col(\"percent_h3s\") > 0.25)\n",
    "        & (pl.col(\"duration\") > 0)\n",
    "    )\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col(c).shift(1).backward_fill(1).over(\"object_id\").alias(f\"{c}_shifted\")\n",
    "            for c in [\"start_time\", \"end_time\"]\n",
    "        ]\n",
    "    )\n",
    "    .filter(pl.col(\"end_time\") >= pl.col(\"end_time_shifted\"))\n",
    "    .groupby([\"object_id\"])\n",
    "    .agg([pl.col(\"clusters\")])\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col('clusters').arr.lengths().alias('num_clusters'),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "to_from_df = (\n",
    "    cluster_df.explode(\"clusters\")\n",
    "    .with_columns(\n",
    "        [pl.col(\"clusters\").shift(-1).over(\"object_id\").alias(\"clusters_shifted\")]\n",
    "    )\n",
    "    .filter(pl.col(\"clusters_shifted\").is_not_null())\n",
    "    .rename({\"clusters\": \"from\", \"clusters_shifted\": \"to\"})\n",
    "    .to_pandas()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df.filter(pl.col('num_clusters') > 15).to_pandas().iloc[0]['clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_from_df = to_from_df.groupby(by=['from', 'to']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.from_pandas_edgelist(\n",
    "    to_from_df, \n",
    "    source=\"from\", \n",
    "    target=\"to\", \n",
    "    edge_attr=\"object_id\",\n",
    "    edge_key=\"object_id\",\n",
    "    create_using=nx.DiGraph\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the Shortest Parth\n",
    "G.get_edge_data(7, 54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the graph\n",
    "nx.draw(G, with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [x for x in G.nodes() if (G.out_degree(x) > 0) & (G.in_degree(x) < 1)]\n",
    "sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinks = [x for x in G.nodes() if (G.out_degree(x) < 1) & (G.in_degree(x) >= 1)]\n",
    "sinks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for source in sources:\n",
    "    for target in sinks:\n",
    "        print(list(nx.simple_paths.all_simple_edge_paths(G, source, target, cutoff=19)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.in_degree(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the clusters\n",
    "fig = go.Figure()\n",
    "\n",
    "for cluster, h3s in cluster_h3s.items():\n",
    "    if cluster == -1:\n",
    "        continue\n",
    "\n",
    "    # if cluster not in sources + sinks:\n",
    "    #     continue\n",
    "    if cluster not in [18, 38, 34, 54,  1, 29, 30,  2, 31,  3,  4, 32, 58, 40,  5, 37, 39, 9,  0]:\n",
    "        continue\n",
    "\n",
    "    color = \"red\" if cluster in sources else \"blue\"\n",
    "\n",
    "    # plot the extend of the h3 cells\n",
    "    geometry = h3.h3_set_to_multi_polygon(h3s, geo_json=True)\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scattermapbox(\n",
    "            lon=[x[0] for x in geometry[0][0]],\n",
    "            lat=[x[1] for x in geometry[0][0]],\n",
    "            mode=\"lines\",\n",
    "            name=f\"cluster {cluster}\",\n",
    "            fill=\"toself\",\n",
    "            fillcolor=color,\n",
    "            line_color=color,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# make the plot square\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=800,\n",
    "    autosize=False,\n",
    "    showlegend=True,\n",
    "    xaxis=dict(\n",
    "        scaleanchor=\"y\",\n",
    "        scaleratio=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# add mapbox\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"open-street-map\",\n",
    "    # center on Tuscaloosa\n",
    "    mapbox_center_lat=33.2,\n",
    "    mapbox_center_lon=-87.5,\n",
    "    mapbox_zoom=10,\n",
    ")\n",
    "\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.h3_resolution = 14\n",
    "\n",
    "interest_df = interest_df.pipe(f.radar_to_h3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_df = interest_df.groupby(\"object_id\").agg(\n",
    "    [\n",
    "        pl.col(\"h3\").unique(),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_hex = (\n",
    "    interest_df.groupby(\"h3\")\n",
    "    .agg(\n",
    "        [\n",
    "            pl.count(\"object_id\").alias(\"count\"),\n",
    "        ]\n",
    "    )\n",
    "    .with_columns(\n",
    "        [\n",
    "            pl.col(\"h3\").apply(lambda x: h3.h3_to_parent(x, 12)).alias(\"h3_12\"),\n",
    "            pl.col(\"h3\").apply(lambda x: h3.h3_to_parent(x, 13)).alias(\"h3_13\"),\n",
    "        ]\n",
    "    )\n",
    "    .sort(\"count\", descending=True)\n",
    "    .groupby([\"h3_13\"])\n",
    "    .agg(\n",
    "        [\n",
    "            pl.col(\"h3\").head(2),\n",
    "            pl.col(\"count\").head(2),\n",
    "        ]\n",
    "    )\n",
    "    .explode([\"h3\", \"count\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a fully connected graph of the h3 cells\n",
    "from networkx import from_pandas_edgelist, Graph\n",
    "\n",
    "\n",
    "h3_product = list(itertools.product(significant_hex[\"h3\"], repeat=2))\n",
    "\n",
    "# make a dataframe of the product\n",
    "h3_product_df = pl.DataFrame(h3_product, schema=[\"h3_1\", \"h3_2\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_parts = (\n",
    "    interest_df.unique(maintain_order=False, subset=[\"h3\", \"object_id\"])\n",
    "    .select([\"h3\", \"object_id\"])\n",
    "    .filter(pl.col(\"h3\").is_in(h3_product_df[\"h3_1\"].to_list()))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_product_df.lazy().join(\n",
    "    vehicle_parts.lazy(),\n",
    "    left_on=\"h3_1\",\n",
    "    right_on=\"h3\",\n",
    "    how=\"inner\",\n",
    ").join(\n",
    "    vehicle_parts.lazy(),\n",
    "    left_on=\"h3_2\",\n",
    "    right_on=\"h3\",\n",
    "    how=\"inner\",\n",
    ").filter(\n",
    "    pl.col(\"object_id_1\") == pl.col(\"object_id_2\")\n",
    ").collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the connections appear in the dataset\n",
    "for h3_1, h3_2 in tqdm(h3_product):\n",
    "    c = traj_df.filter(pl.col(\"h3\").is_in([h3_1, h3_2])).shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interest_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Trajectories Themselves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hausdorff(u, v):\n",
    "    return max(directed_hausdorff(u, v)[0], directed_hausdorff(v, u)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "veh_df = interest_df.groupby([\"object_id\", \"h3\"]).first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles_ = (\n",
    "    veh_df.with_columns(\n",
    "        pl.struct(\n",
    "            [\n",
    "                \"f32_positionX_m\",\n",
    "                \"f32_positionY_m\",\n",
    "            ]\n",
    "        )\n",
    "        .apply(lambda x: [x[\"f32_positionX_m\"], x[\"f32_positionY_m\"]])\n",
    "        .alias(\"position\")\n",
    "    )\n",
    "    .groupby(\"object_id\")\n",
    "    .agg([\"position\"])\n",
    "    .sample(100)[\"position\"]\n",
    "    .to_list()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the pairwise distance matrix\n",
    "import similaritymeasures\n",
    "\n",
    "distance_matrix = np.zeros((len(vehicles_), len(vehicles_)))\n",
    "\n",
    "for i, u in enumerate(vehicles_):\n",
    "    for j in range(i + 1, len(vehicles_)):\n",
    "        distance_matrix[i, j] = similaritymeasures.frechet_dist(u, vehicles_[j])\n",
    "        distance_matrix[j, i] = distance_matrix[i, j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = DBSCAN(eps=100, min_samples=2, metric=\"precomputed\")\n",
    "cl.fit(distance_matrix)\n",
    "\n",
    "# add the cluster labels to the tsne dataframe\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Sub Trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the cluster labels and vehicle positions\n",
    "cluster_df = pl.DataFrame(\n",
    "    {\n",
    "        \"cluster\": cl.labels_,\n",
    "        \"position\": vehicles_,\n",
    "    }\n",
    ")\n",
    "\n",
    "cluster_df[\"cluster\"].value_counts()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "for cluster, c_df in cluster_df.groupby(\"cluster\"):\n",
    "    v = c_df[\"position\"].to_list()\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[x[0] for k in v for x in k],\n",
    "            y=[x[1] for k in v for x in k],\n",
    "            mode=\"markers\",\n",
    "            name=f\"cluster {cluster}\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# next steps:\n",
    "\n",
    "- use the convex hull to get the shape of the cluster\n",
    "- find overlaps and merge clusters\n",
    "- if there is three or more clusters, it is a junction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df[\"cluster\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_df[\"count\"].median()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_df.loc[h3_df[\"count\"] >= 6].h3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the file paths\n",
    "network_outline_file = repo_root / \"geo_data\" / \"network_outline.geojson\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "with open(network_outline_file, \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "ls = Polygon(json_data[\"features\"][0][\"geometry\"][\"coordinates\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_network = ox.graph_from_polygon(ls)\n",
    "ox.plot_graph(road_network)\n",
    "\n",
    "# project to UTM\n",
    "road_network = ox.project_graph(road_network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for e in road_network.edges:\n",
    "    # get the nodes\n",
    "    u, v, _ = e\n",
    "    # get the node attributes\n",
    "    u_data = road_network.nodes[u]\n",
    "    v_data = road_network.nodes[v]\n",
    "\n",
    "    # get the midpoint between the nodes\n",
    "    pp = (u_data[\"x\"] + v_data[\"x\"]) / 2, (u_data[\"y\"] + v_data[\"y\"]) / 2\n",
    "    # get the angle\n",
    "    angle = np.arctan2(v_data[\"y\"] - u_data[\"y\"], v_data[\"x\"] - u_data[\"x\"])\n",
    "    angle += np.pi / 2\n",
    "\n",
    "    # draw a line perpendicular to the edge\n",
    "    p1 = pp[0] + np.cos(angle) * 10, pp[1] + np.sin(angle) * 10\n",
    "    p2 = pp[0] - np.cos(angle) * 10, pp[1] - np.sin(angle) * 10\n",
    "\n",
    "    #  add some width to the polygon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radar-trajectories",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9df5da3677d8bc0572278c4b47e94911772adde63d7d6fd5448fdb0b10af7082"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
